import org.apache.spark.sql.SparkSession

object Clean {
  def main(args: Array[String]): Unit = {
    // Initialize Spark session
    val spark = SparkSession.builder
      .appName("Clean")
      .getOrCreate()

    // Read command line arguments for CSV file path and separator
    if (args.length < 1) {
      println("Please provide the path to the CSV file.")
      System.exit(1)
    }
    val csvFilePath = args(0)
    val separator = if (args.length >= 2) args(1) else ","

    // Read CSV file from HDFS
    val df = spark.read
      .option("header", "true")
      .option("inferSchema", "true")
      .option("sep", separator)
      .option("multiLine", "true") // Read records that span multiple lines
      .option("ignoreLeadingWhiteSpace", "true") // Ignore leading white spaces in each field
      .option("ignoreTrailingWhiteSpace", "true") // Ignore trailing white spaces in each field
      .option("nullValue", "\"\"") // Treat empty double quotes as null values
      .csv(csvFilePath)

    
  }
}
